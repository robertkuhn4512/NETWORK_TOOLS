# For production builds I am going to use a single fqdn for the project.
# On my dev setup I will be using networkengineertools.com which I will have
# Pointed to my local VM. So anywhere you see a reference to that you can change it to
# Your preferred domain. or hardcode this on your dev machine if you want.
#
# Items like keycloak will need a stationary name to make sure things flow smoothly and work correctly.


name: network_tools

volumes:
  postgres_data:
    name: network_tools_postgres_data
  postgres_backups:
    name: network_tools_postgres_backups
  postgres_certs:
    name: network_tools_postgres_certs
  postgres_vault_rendered:
    name: network_tools_postgres_vault_rendered
  keycloak_vault_rendered:
    name: network_tools_keycloak_vault_rendered
  pgadmin_certs:
    name: network_tools_pgadmin_certs
  fastapi_vault_rendered:
    name: network_tools_vault_rendered
  redis_data:
    name: network_tools_redis_data
  flower_data:
    name: network_tools_flower_data

networks:
  db_net:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.30.20.0/24
  public_net:
    driver: bridge  # Standard bridge allows port mapping

services:
  # From the project root (~/NETWORK_TOOLS) to start all containers
  # docker compose -f docker-compose.prod.yml up -d
  #
  # Rebuild command if needed
  # docker compose -f docker-compose.prod.yml up -d --build
  #
  # If you only want to start the Vault service (named vault_production_node)
  # docker compose -f docker-compose.prod.yml up -d vault_production_node
  #
  # Monitor the log file while the container is running (It must be brought up and running in order to populate the log file)
  # docker compose -p network_tools -f docker-compose.prod.yml logs --tail 200 -f --timestamps vault_production_node
  #
  #

  # NGINX Container
  # docker compose -f docker-compose.prod.yml up -d nginx_gateway
  #
  # Rebuild without changing dependencies
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate nginx_gateway
  # docker logs --tail 200 -f nginx_gateway
  nginx_gateway:
    image: nginx:1.27-alpine
    container_name: nginx_gateway
    restart: unless-stopped
    depends_on:
       - keycloak
       - vault_production_node
       - fastapi_api
       - pgadmin
       - celery_worker
       - flower
    networks:
      public_net:
    ports:
      - "${PUBLIC_HTTP_PORT}:8080"
      - "${PUBLIC_HTTPS_PORT}:8443"
    read_only: true
    user: "0:0"
    tmpfs:
      - /etc/nginx/conf.d:rw,nosuid,nodev,noexec,mode=0755,uid=0,gid=0
      - /var/cache/nginx:rw,nosuid,nodev,noexec,mode=0755,uid=0,gid=0
      - /var/run:rw,nosuid,nodev,noexec,mode=0755,uid=0,gid=0
      - /tmp:rw,nosuid,nodev,noexec,mode=1777,uid=0,gid=0
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN             # Required for entrypoint to adjust permissions if needed
      - SETGID            # Required for nginx to drop privileges
      - SETUID            # Required for nginx to drop privileges
      #- NET_BIND_SERVICE  # This is NOT needed for 8080/8443 inside the container.
                          # Add it only if you change nginx to listen on 80/443 inside the container.
    volumes:
      - ./backend/app/nginx/templates:/etc/nginx/templates:ro
      - ./backend/app/nginx/certs:/etc/nginx/certs:ro
    environment:
      PRIMARY_SERVER_FQDN: "${PRIMARY_SERVER_FQDN}"
      PUBLIC_HTTPS_PORT: "${PUBLIC_HTTPS_PORT}"
      NGINX_ENVSUBST_OUTPUT_DIR: "/etc/nginx/conf.d" # Explicitly tell it where to write
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080/nginx-health >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 10

  vault_production_node:
    networks:
      public_net:
    image: hashicorp/vault:1.21.1
    hostname: vault_production_node
    container_name: vault_production_node
    env_file:
      - .env      # Populate any important production environment variables here.
                  # See the file environment_variable_guide.md for information on what variables to populate
                  # Rootless: publish ports instead of host networking
    ports:
      - "8200:8200"
      - "8201:8201"   # There is no need to publish this port unless you are running multiple nodes and they
                      # Need to sync to each other. In either case, It's best practice to also firewall each
                      # Port to the servers that need to talk to each other.
    expose:
      - "8200"
      - "8201"
    cap_drop:
      - ALL
    cap_add:
      - IPC_LOCK  # required for memory locking
    security_opt:
      - no-new-privileges:true
    # Resource guardrails (tune to your host)
    pids_limit: 256
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      core: 0  # disables core dumps at container level (defense-in-depth)
    restart: unless-stopped
    volumes:
      - ./backend/app/security/configuration_files/vault/config/vault_configuration_primary_node.hcl:/vault/config/config.hcl:ro

      # TLS material (read-only)
      - ./backend/app/nginx/certs/cert.crt:/vault/certs/cert.crt:ro
      - ./backend/app/nginx/certs/cert.key:/vault/certs/cert.key:ro

      # Data/logs (must be writable by the container user)
      - ./container_data/vault/data:/vault/data
      - ./container_data/vault/data/logs:/vault/logs
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    read_only: true
    tmpfs:
      - /tmp:rw,noexec,nosuid,size=64m
      - /run:rw,noexec,nosuid,size=64m
    environment:
      VAULT_DISABLE_MLOCK: "true"                                                           # Prefer enabling mlock; if you cannot, keep it true and harden host swap settings.
      VAULT_UI: "true"
      VAULT_CACERT: "/vault/certs/cert.crt"                                                 # VAULT_CACERT should normally point to a CA certificate (e.g., ca.crt),
                                                                                            # not the server leaf certâ€”unless your leaf cert is self-signed and acting as CA. Make this explicit by
                                                                                            # mounting a CA file and pointing to it.
      #VAULT_ADDR: "https://vault_production_node:8200"                                     # For dev container use (Hash / remove / just use the value below and change the value below in the .env file)
                                                                                            # This entry refers to the containers name. Which because of docker is in a hosts entry
                                                                                            # So reachability internally will work with this. Externally you would need to add an antry
                                                                                            # for it on your dev machine if it's separate from the machine hosting the container.
      VAULT_ADDR: "https://vault.${PRIMARY_SERVER_FQDN}:8200"                               # For production use with a valid FQDN (Hash the above, unhash this and rebuild)
                                                                                            # This should be populated in the environment file SERVER_1_FQDN_FULL="Your domain name here"
      VAULT_API_ADDR: "https://vault.${PRIMARY_SERVER_FQDN}:8200"
      VAULT_CLUSTER_ADDR: "https://vault.${PRIMARY_SERVER_FQDN}:8201"
      #VAULT_LOG_LEVEL: "debug"                                                              # Used for debugging vault while you are testing. Comment out and rebuild with
                                                                                            # VAULT_LOG_LEVEL: "info" for production use
      VAULT_LOG_LEVEL: "info"
      VAULT_RAFT_NODE_ID: "Node1"                                                           # If you ever change this and reboot a node in an existing cluster you will have issues
                                                                                            # So it's better to pick a name and keep it
    entrypoint: ["/bin/sh","-lc","exec vault server -config=/vault/config/config.hcl"]

  # docker compose -f docker-compose.prod.yml up -d vault_agent_postgres_pgadmin
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate vault_agent_postgres_pgadmin
  vault_agent_postgres_pgadmin:
    image: hashicorp/vault:1.21.1
    container_name: vault_agent_postgres_pgadmin
    restart: unless-stopped
    depends_on:
      - vault_production_node
    entrypoint: [ "/bin/sh","-lc","exec vault agent -config=/vault/agent/agent.hcl" ]
    environment:
      PRIMARY_SERVER_FQDN: "${PRIMARY_SERVER_FQDN:-vault_production_node}"
      VAULT_ADDR: "https://${PRIMARY_SERVER_FQDN:-vault_production_node}:8200"
      VAULT_CACERT: "/vault/ca/ca.crt"
    read_only: true
    tmpfs:
      - /tmp
      - /run
    volumes:
      # Trust bundle for Vault TLS (prefer CA if available)
      - ./backend/app/security/configuration_files/vault/certs/ca.crt:/vault/ca/ca.crt:ro

      # Agent config + templates
      - ./backend/app/postgres/vault_agent/agent.hcl:/vault/agent/agent.hcl:ro
      - ./backend/app/postgres/vault_agent/templates:/vault/templates:ro

      # Host-exported AppRole material
      - ./container_data/vault/approle/postgres_pgadmin_agent:/vault/approle:ro

      # Render target (shared)
      - postgres_vault_rendered:/vault/rendered
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    healthcheck:
      test: [ "CMD-SHELL", "test -s /vault/rendered/postgres_db && test -s /vault/rendered/postgres_user && test -s /vault/rendered/postgres_password && test -s /vault/rendered/pgadmin_password && test -s /vault/rendered/servers.json" ]
      interval: 5s
      timeout: 3s
      retries: 30

  # docker compose -f docker-compose.prod.yml up -d vault_agent_keycloak
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate vault_agent_keycloak
  vault_agent_keycloak:
    image: hashicorp/vault:1.21.1
    container_name: vault_agent_keycloak
    restart: unless-stopped
    depends_on:
      - vault_production_node
    entrypoint:
      [ "/bin/sh","-lc","mkdir -p /vault/rendered/tls && exec vault agent -config=/vault/agent/agent.hcl" ]
    environment:
      PRIMARY_SERVER_FQDN: "${PRIMARY_SERVER_FQDN:-vault_production_node}"
      VAULT_ADDR: "https://${PRIMARY_SERVER_FQDN:-vault_production_node}:8200"
      VAULT_CACERT: "/vault/ca/ca.crt"
    tmpfs:
      - /tmp
      - /run
    volumes:
      - ./backend/app/security/configuration_files/vault/certs/ca.crt:/vault/ca/ca.crt:ro
      - ./backend/app/keycloak/vault_agent/agent.hcl:/vault/agent/agent.hcl:ro
      - ./backend/app/keycloak/vault_agent/templates:/vault/templates:ro
      - ./container_data/vault/approle/keycloak_agent:/vault/approle:ro
      - keycloak_vault_rendered:/vault/rendered
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    healthcheck:
      test: [ "CMD-SHELL", "test -s /vault/rendered/keycloak.env" ]
      interval: 5s
      timeout: 3s
      retries: 30


  # docker compose -f docker-compose.prod.yml up -d keycloak
  #
  # Force rebuild without changing dependant containers (Prefer this, otherwise it may cause issues with the vault agent container using up your seret_id uses)
  # docker compose -f docker-compose.prod.yml up -d keycloak --force-recreate --no-deps keycloak
  # docker logs --tail 200 -f keycloak
  #
  # Once nginx is up
  # https://auth.networkengineertools.com:8443
  #
  keycloak:
    networks:
      db_net:
        ipv4_address: 172.30.20.30
      public_net:
    image: quay.io/keycloak/keycloak:26.4.7
    container_name: keycloak
    restart: unless-stopped
    depends_on:
      vault_agent_keycloak:
        condition: service_healthy
      postgres_primary:
        condition: service_started
    volumes:
      - keycloak_vault_rendered:/run/vault:ro
      - ./backend/app/keycloak/bin/keycloak_entrypoint_from_vault.sh:/opt/keycloak/bin/keycloak_entrypoint_from_vault.sh:ro
    entrypoint: [ "/bin/bash", "/opt/keycloak/bin/keycloak_entrypoint_from_vault.sh" ]
    command: [ "start" ]
    environment:
      KC_PROXY_HEADERS: "xforwarded"
      KC_HTTP_RELATIVE_PATH: "/"
      KC_HTTP_ENABLED: "true"
      KC_HTTP_MANAGEMENT_SCHEME: "http"
      KC_HOSTNAME: "https://auth.${PRIMARY_SERVER_FQDN}:${PUBLIC_HTTPS_PORT}"
      KC_HOSTNAME_STRICT: "true"
      KC_HOSTNAME_ADMIN: "https://auth.${PRIMARY_SERVER_FQDN}:${PUBLIC_HTTPS_PORT}"
      KC_HOSTNAME_DEBUG: "true"
      KC_HEALTH_ENABLED: "true"
      KC_METRICS_ENABLED: "true"
      KC_PROXY_TRUSTED_ADDRESSES: "172.30.0.0/16,172.30.20.0/24"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"

  # docker compose -f docker-compose.prod.yml up -d postgres_certs_init

  #postgres_certs_init:
  #  image: postgres:18.1
  #  container_name: postgres_certs_init
  #  user: "0:0"
  #  restart: "no"
  #  volumes:
  #    - ./backend/app/postgres/certs:/src:ro
  #    - postgres_certs:/dest
  #  tmpfs:
  #    - /tmp
  #    - /run
  #  logging:
  #    driver: json-file
  #    options:
  #      max-size: "50m"
  #      max-file: "5"
  #  entrypoint:
  #    - /bin/sh
  #    - -lc
  #    - |
  #      set -e
  #      mkdir -p /dest
  #      cp -f /src/cert.crt /dest/server.crt
  #      cp -f /src/cert.key /dest/server.key
  #      cp -f /src/ca.crt   /dest/ca.crt
  #      chmod 600 /dest/server.key
  #      chmod 644 /dest/server.crt /dest/ca.crt
  #      chown -R 999:999 /dest || true
  #      ls -l /dest

  # Not needed for the current build. Leaving in for reference.
  # Currently nginx will proxy to the containers frontend so that will be handling the tls certs
  # This will need to be uncommented as well as the settings on the container itself referencing it if
  # you intend on using it on it's own
  #
  #pgadmin_certs_init:
  #  image: postgres:18.1
  #  container_name: pgadmin_certs_init
  #  user: "0:0"
  #  restart: "no"
  #  volumes:
  #    - ./backend/app/pgadmin/certs:/src:ro
  #    - pgadmin_certs:/dest
  #  tmpfs:
  #    - /tmp
  #    - /run
  #  entrypoint:
  #    - /bin/sh
  #    - -lc
  #    - |
  #      set -e
  #      mkdir -p /dest
  #      cp -f /src/cert.crt /dest/server.cert
  #      cp -f /src/cert.key /dest/server.key
  #      cp -f /src/ca.crt   /dest/ca.crt
  #      chmod 600 /dest/server.key
  #      chmod 644 /dest/server.cert /dest/ca.crt
  #      chown -R 5050:5050 /dest || true
  #      ls -l /dest

  # docker compose -f docker-compose.prod.yml up -d postgres_primary
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate postgres_primary
  postgres_primary:
    # REMOVE this for production:
    # ports:
    #   - "5432:5432"
    # Optional for local-only admin:
    ports:
      - "5432:5432"
    networks:
      db_net:
        ipv4_address: 172.30.20.10
    image: postgres:18.1
    container_name: postgres_primary
    restart: unless-stopped
    env_file:
      - .env
    depends_on:
      #postgres_certs_init:
      #  condition: service_completed_successfully
      vault_agent_postgres_pgadmin:
        condition: service_healthy
    environment:
      POSTGRES_DB_FILE: "/run/vault/postgres_db"
      POSTGRES_USER_FILE: "/run/vault/postgres_user"
      POSTGRES_PASSWORD_FILE: "/run/vault/postgres_password"
      PGDATA: "${PGDATA:-/var/lib/postgres/data/pgdata}"
    volumes:
      - postgres_data:/var/lib/postgres/data
      #- postgres_certs:/etc/postgres/certs:ro
      - ./backend/app/postgres/config/postgres.conf:/etc/postgres/postgres.conf:ro
      - ./backend/app/postgres/config/pg_hba.conf:/etc/postgres/pg_hba.conf:ro

      # rendered secrets from vault agent
      - postgres_vault_rendered:/run/vault:ro

      # Initial build for network_tools tables
      - ./backend/app/postgres/init/20_network_tools_schema.sql:/docker-entrypoint-initdb.d/20_network_tools_schema.sql:ro

      - ./container_data/postgres_primary:/var/lib/postgresql/data
      - ./container_data/postgres_backups:/backups
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    command: ["postgres", "-c", "config_file=/etc/postgres/postgres.conf", "-c", "hba_file=/etc/postgres/pg_hba.conf"]

  # docker compose -f docker-compose.prod.yml up -d pgadmin
  # docker compose -f docker-compose.prod.yml up -d pgadmin --build
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate pgadmin
  pgadmin:
    networks:
      db_net:
        ipv4_address: 172.30.20.20
      public_net:
    image: dpage/pgadmin4:9.11
    container_name: pgadmin
    restart: unless-stopped
    depends_on:
      #pgadmin_certs_init:
      #  condition: service_completed_successfully
      vault_agent_postgres_pgadmin:
        condition: service_healthy
      postgres_primary:
        condition: service_started
    env_file:
      - .env
    environment:
      PGADMIN_DEFAULT_EMAIL: "${PGADMIN_DEFAULT_EMAIL}"
      PGADMIN_DEFAULT_PASSWORD_FILE: "/run/vault/pgadmin_password"
      PGADMIN_SERVER_JSON_FILE: "/run/vault/servers.json"
      PGADMIN_REPLACE_SERVERS_ON_STARTUP: "True"
      PGDATA: "/var/lib/postgres/data/pgdata"
      PGADMIN_CONFIG_ENABLE_SERVER_PASS_EXEC_CMD: "True" # This "Should" be disabled if you are using multiple user logins to
                                                         # Access postgres as this will configure access to the postgres server with
                                                         # The root account. For now, i'm leaving it enabled for testing.
    volumes:
      - postgres_vault_rendered:/run/vault:ro
      # Not needed but left for an example - Running this behind the nginx service
      # - pgadmin_certs:/certs:ro
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    healthcheck:
      test: [ "CMD-SHELL",
        "test -s 
              && test -s /run/vault/postgres_db \
              && test -s /run/vault/postgres_user \
              && test -s /run/vault/postgres_password \
              && test -s /run/vault/pgadmin_password \
              && test -s /run/vault/servers.json"
      ]
      interval: 5s
      timeout: 3s
      retries: 30

  # docker compose -f docker-compose.prod.yml up -d vault_agent_fastapi
  # docker compose -f docker-compose.prod.yml up -d vault_agent_fastapi --build
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate vault_agent_fastapi

  vault_agent_fastapi:
    image: hashicorp/vault:1.21.1
    container_name: vault_agent_fastapi
    restart: unless-stopped
    depends_on:
      - vault_production_node
    entrypoint: [ "/bin/sh","-lc","exec vault agent -config=/vault/agent/agent.hcl" ]
    environment:
      PRIMARY_SERVER_FQDN: "${PRIMARY_SERVER_FQDN:-vault_production_node}"
      VAULT_ADDR: "https://${PRIMARY_SERVER_FQDN:-vault_production_node}:8200"
      VAULT_CACERT: "/vault/ca/ca.crt"
    read_only: true
    tmpfs:
      - /tmp
      - /run
    volumes:
      - ./backend/app/security/configuration_files/vault/certs/ca.crt:/vault/ca/ca.crt:ro
      - ./backend/app/fastapi/vault_agent/agent.hcl:/vault/agent/agent.hcl:ro
      - ./backend/app/fastapi/vault_agent/templates:/vault/templates:ro
      - ./container_data/vault/approle/fastapi_agent:/vault/approle:ro
      - fastapi_vault_rendered:/vault/rendered
    healthcheck:
      test: [ "CMD-SHELL", "test -s /vault/rendered/fastapi_secrets.json" ]
      interval: 5s
      timeout: 3s
      retries: 30

  # ------------------------------------------------------------
  # FastAPI API container (loads ALL Vault keys into process env)
  # ------------------------------------------------------------
  # docker compose -f docker-compose.prod.yml up -d fastapi_api
  # docker compose -f docker-compose.prod.yml up -d fastapi_api --build
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate fastapi_api
  fastapi_api:
    image: network_tools_fastapi:latest
    build:
      context: ./backend/app/fastapi
      dockerfile: Dockerfile
    container_name: fastapi_api
    restart: unless-stopped
    depends_on:
      vault_agent_fastapi:
        condition: service_healthy
      postgres_primary:
        condition: service_healthy
      redis:
        condition: service_healthy
      keycloak:
        condition: service_healthy
    networks:
      db_net:
        ipv4_address: 172.30.20.40
      public_net:
    ports:
      - "8000:8000"
    volumes:
      - fastapi_vault_rendered:/run/vault:ro
      # TLS material (read-only)
      - ./backend/app/nginx/certs/ca.crt:/run/certs/networktools_ca.crt:ro
      - ./container_data/logs/fastapi:/var/log/network_tools/fastapi
      - ./container_data/backups/device_configuration_backups:/backups/device_configuration_backups
    environment:
      VAULT_TOKEN_FILE: "/run/vault/.vault-token"
      VAULT_ADDR: "https://vault.${PRIMARY_SERVER_FQDN}:8200"
      VAULT_SECRETS_JSON: "/run/vault/fastapi_secrets.json"
      APP_MODULE: "app.main:app"
      BIND_HOST: "0.0.0.0"
      BIND_PORT: "8000"
      WEB_CONCURRENCY: "2"
      PRIMARY_SERVER_FQDN: "${PRIMARY_SERVER_FQDN}"
      SSL_CERT_FILE: /run/certs/networktools_ca.crt
      FASTAPI_OIDC_CA_BUNDLE: /run/certs/networktools_ca.crt
    command: [ "python", "/app/run_server.py" ]
    healthcheck:
      test: [ "CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://127.0.0.1:8000/health\", timeout=2).read()'" ]
      interval: 10s
      timeout: 3s
      retries: 10

  # ------------------------------------------------------------
  # Redis (Celery broker/backend)
  # ------------------------------------------------------------
  # docker compose -f docker-compose.prod.yml up -d redis
  # docker compose -f docker-compose.prod.yml up -d redis --build
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate redis
  redis:
    image: redis:8.4-alpine
    container_name: redis
    restart: unless-stopped
    networks:
      db_net:
        ipv4_address: 172.30.20.50   # optional; keep consistent with your scheme
    volumes:
      - redis_data:/data
      - fastapi_vault_rendered:/run/vault:ro
    command: ["redis-server", "/run/vault/redis.conf"]
    healthcheck:
      test: [ "CMD-SHELL", 'PASS="$$(tr -d "\r\n" </run/vault/redis_password)"; REDISCLI_AUTH="$$PASS" redis-cli ping | grep -q PONG' ]
      interval: 10s
      timeout: 5s
      retries: 10

  # ------------------------------------------------------------
  # Celery worker (uses the same Vault-rendered JSON as FastAPI)
  # ------------------------------------------------------------
  # docker compose -f docker-compose.prod.yml up -d celery_worker
  # docker compose -f docker-compose.prod.yml up -d celery_worker --build
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate celery_worker
  celery_worker:
    image: network_tools_fastapi:latest
    container_name: celery_worker
    restart: unless-stopped
    networks:
      db_net:
        ipv4_address: 172.30.20.60
      public_net:
    depends_on:
      vault_agent_fastapi:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres_primary:
        condition: service_started
    volumes:
      - fastapi_vault_rendered:/run/vault:ro
      - ./container_data/logs/celery:/var/log/network_tools/celery
      - ./container_data/backups/device_configuration_backups:/backups/device_configuration_backups
      - ./backend/app/nginx/certs/ca.crt:/run/certs/networktools_ca.crt:ro
    environment:
      VAULT_ENV_OVERRIDE: "0"
      VAULT_TOKEN_FILE: "/run/vault/.vault-token"
      VAULT_CACERT: "/run/certs/networktools_ca.crt"
      VAULT_ADDR: "https://vault.${PRIMARY_SERVER_FQDN}:8200"
      VAULT_SECRETS_JSON: "/run/vault/fastapi_secrets.json"
      LOG_DIR: "/var/log/network_tools/celery"
      LOG_FILE: "network_tools_celery.log"
      LOG_LEVEL: "DEBUG"
      LOG_TO_STDOUT: "1"
    command:
      - "python"
      - "/app/vault_env_exec.py"
      - "celery"
      - "-A"
      - "app.celery_app:celery_app"
      - "worker"
      - "--loglevel=DEBUG"
      - "--concurrency=10"

  # ------------------------------------------------------------
  # flower - Frontend for celery
  # ------------------------------------------------------------
  # docker compose -f docker-compose.prod.yml up -d flower
  # docker compose -f docker-compose.prod.yml up -d flower --build
  # docker compose -f docker-compose.prod.yml up -d --no-deps --build --force-recreate flower
  flower:
    image: network_tools_fastapi:latest
    container_name: flower
    restart: unless-stopped
    depends_on:
      vault_agent_fastapi:
        condition: service_healthy
      redis:
        condition: service_healthy
      celery_worker:
        condition: service_started
    networks:
      db_net:
      public_net:
    # Expose only to other containers (nginx), not the host
    expose:
      - "5555"
    volumes:
      - fastapi_vault_rendered:/run/vault:ro
      - flower_data:/data
    environment:
      VAULT_SECRETS_JSON: "/run/vault/fastapi_secrets.json"
    command:
      - "python"
      - "/app/vault_env_exec.py"
      - "celery"
      - "-A"
      - "app.celery_app:celery_app"
      - "flower"
      - "--address=0.0.0.0"
      - "--port=5555"
      - "--persistent=True"
      - "--db=/data/flower.db"
      # If you proxy under a path (example: https://host/flower/), enable:
      # - "--url_prefix=/flower"
    healthcheck:
      test: [ "CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://127.0.0.1:5555/\", timeout=2).read()' >/dev/null" ]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 15s
